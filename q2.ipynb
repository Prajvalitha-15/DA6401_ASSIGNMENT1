{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "creating run (0.3s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\prajv\\Downloads\\ass1\\DA6401_ASSIGNMENT1\\wandb\\run-20250309_183635-ayubj2te</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da6401-assignments/assignment1/runs/ayubj2te' target=\"_blank\">giddy-violet-5</a></strong> to <a href='https://wandb.ai/da6401-assignments/assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da6401-assignments/assignment1' target=\"_blank\">https://wandb.ai/da6401-assignments/assignment1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da6401-assignments/assignment1/runs/ayubj2te' target=\"_blank\">https://wandb.ai/da6401-assignments/assignment1/runs/ayubj2te</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/da6401-assignments/assignment1/runs/ayubj2te?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1ca7af9d160>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize WandB\n",
    "wandb.init(project=\"assignment1\", entity=\"da6401-assignments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "config = wandb.config\n",
    "config.learning_rate = 0.01\n",
    "config.epochs = 10\n",
    "config.batch_size = 64\n",
    "config.hidden_layers = [256,128, 64]  # Two hidden layers with 128 and 64 neurons\n",
    "config.input_size = 784  # 28x28 images\n",
    "config.output_size = 10  # 10 classes for Fashion-MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Fashion-MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# Normalize the data\n",
    "x_train = x_train.reshape(-1, config.input_size) / 255.0\n",
    "x_test = x_test.reshape(-1, config.input_size) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 2.3080\n",
      "Epoch 2/10, Loss: 2.3109\n",
      "Epoch 3/10, Loss: 2.3008\n",
      "Epoch 4/10, Loss: 2.3058\n",
      "Epoch 5/10, Loss: 2.3015\n",
      "Epoch 6/10, Loss: 2.3029\n",
      "Epoch 7/10, Loss: 2.3058\n",
      "Epoch 8/10, Loss: 2.3043\n",
      "Epoch 9/10, Loss: 2.3005\n",
      "Epoch 10/10, Loss: 2.3043\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>loss</td><td>▆█▁▅▂▃▅▄▁▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>loss</td><td>2.30433</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">giddy-violet-5</strong> at: <a href='https://wandb.ai/da6401-assignments/assignment1/runs/ayubj2te' target=\"_blank\">https://wandb.ai/da6401-assignments/assignment1/runs/ayubj2te</a><br> View project at: <a href='https://wandb.ai/da6401-assignments/assignment1' target=\"_blank\">https://wandb.ai/da6401-assignments/assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250309_183635-ayubj2te\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class FeedforwardNeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_layers, output_size):\n",
    "        \"\"\"\n",
    "        Initialize the neural network.\n",
    "        :param input_size: Number of input features (e.g., 784 for 28x28 images).\n",
    "        :param hidden_layers: List containing the number of neurons in each hidden layer.\n",
    "        :param output_size: Number of output classes (e.g., 10 for Fashion-MNIST).\n",
    "        \"\"\"\n",
    "        self.layers = []\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "\n",
    "        # Input layer to first hidden layer\n",
    "        prev_size = input_size\n",
    "        for layer_size in hidden_layers:\n",
    "            self.weights.append(np.random.randn(prev_size, layer_size) * 0.01)\n",
    "            self.biases.append(np.zeros((1, layer_size)))\n",
    "            prev_size = layer_size\n",
    "\n",
    "        # Hidden layers to output layer\n",
    "        self.weights.append(np.random.randn(prev_size, output_size) * 0.01)\n",
    "        self.biases.append(np.zeros((1, output_size)))\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"Sigmoid activation function.\"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def softmax(self, x):\n",
    "        \"\"\"Softmax activation function.\"\"\"\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Perform a forward pass through the network.\n",
    "        :param x: Input data.\n",
    "        :return: Output probabilities.\n",
    "        \"\"\"\n",
    "        self.layers = [x]\n",
    "        for w, b in zip(self.weights[:-1], self.biases[:-1]):\n",
    "            x = self.sigmoid(np.dot(x, w) + b)\n",
    "            self.layers.append(x)\n",
    "\n",
    "        # Output layer with softmax\n",
    "        output = self.softmax(np.dot(x, self.weights[-1]) + self.biases[-1])\n",
    "        self.layers.append(output)\n",
    "        return output\n",
    "\n",
    "    def train(self, x_train, y_train, epochs=10, batch_size=64, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Train the network.\n",
    "        :param x_train: Training data.\n",
    "        :param y_train: Training labels.\n",
    "        :param epochs: Number of training epochs.\n",
    "        :param batch_size: Batch size for mini-batch gradient descent.\n",
    "        :param learning_rate: Learning rate for weight updates.\n",
    "        \"\"\"\n",
    "        # One-hot encode the labels\n",
    "        y_train_onehot = np.eye(10)[y_train]\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle the data\n",
    "            indices = np.arange(x_train.shape[0])\n",
    "            np.random.shuffle(indices)\n",
    "            x_train = x_train[indices]\n",
    "            y_train_onehot = y_train_onehot[indices]\n",
    "\n",
    "            # Mini-batch gradient descent\n",
    "            for i in range(0, x_train.shape[0], batch_size):\n",
    "                x_batch = x_train[i:i + batch_size]\n",
    "                y_batch = y_train_onehot[i:i + batch_size]\n",
    "\n",
    "                # Forward pass\n",
    "                predictions = self.forward(x_batch)\n",
    "\n",
    "                # Compute loss (categorical cross-entropy)\n",
    "                loss = -np.mean(np.sum(y_batch * np.log(predictions + 1e-8), axis=1))\n",
    "\n",
    "                # Backward pass (gradient computation)\n",
    "                gradients = []\n",
    "                delta = predictions - y_batch\n",
    "                for l in range(len(self.weights) - 1, -1, -1):\n",
    "                    grad_w = np.dot(self.layers[l].T, delta) / batch_size\n",
    "                    grad_b = np.sum(delta, axis=0, keepdims=True) / batch_size\n",
    "                    gradients.insert(0, (grad_w, grad_b))\n",
    "\n",
    "                    if l > 0:\n",
    "                        delta = np.dot(delta, self.weights[l].T) * self.layers[l] * (1 - self.layers[l])\n",
    "\n",
    "                # Update weights and biases\n",
    "                for l in range(len(self.weights)):\n",
    "                    self.weights[l] -= learning_rate * gradients[l][0]\n",
    "                    self.biases[l] -= learning_rate * gradients[l][1]\n",
    "\n",
    "            # Log metrics to WandB\n",
    "            wandb.log({\"epoch\": epoch + 1, \"loss\": loss})\n",
    "\n",
    "            # Print loss for the epoch\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}\")\n",
    "network = FeedforwardNeuralNetwork(config.input_size, config.hidden_layers, config.output_size)\n",
    "network.train(x_train, y_train, epochs=config.epochs, batch_size=config.batch_size, learning_rate=config.learning_rate)\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
