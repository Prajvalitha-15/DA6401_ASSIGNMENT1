{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import wandb\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((784, 54000), (10, 54000))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the data\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# Normalize the data\n",
    "x_train = x_train.reshape(-1, 784) / 255.0\n",
    "x_test = x_test.reshape(-1, 784) / 255.0\n",
    "\n",
    "# One-hot encode labels\n",
    "y_train, y_test = to_categorical(y_train), to_categorical(y_test)\n",
    "\n",
    "# Split validation data\n",
    "val_split = int(0.1 * x_train.shape[0])\n",
    "\n",
    "# Create validation data\n",
    "x_val, y_val = x_train[:val_split], y_train[:val_split]\n",
    "x_train, y_train = x_train[val_split:], y_train[val_split:]\n",
    "\n",
    "x_train = x_train.T\n",
    "y_train = y_train.T\n",
    "x_val = x_val.T\n",
    "y_val = y_val.T\n",
    "x_test = x_test.T\n",
    "y_test = y_test.T\n",
    "\n",
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class FeedforwardNeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_layers, output_size):\n",
    "        \"\"\"\n",
    "        Initialize the neural network.\n",
    "        :param input_size: Number of input features.\n",
    "        :param hidden_layers: List containing the number of neurons in each hidden layer.\n",
    "        :param output_size: Number of output classes.\n",
    "        \"\"\"\n",
    "        self.layers = []\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.velocities_w = []  # For momentum-based optimizers\n",
    "        self.velocities_b = []\n",
    "        self.squared_w = []  # For RMSprop/Adam\n",
    "        self.squared_b = []\n",
    "        self.moments_w = []  # For Nadam\n",
    "        self.moments_b = []\n",
    "\n",
    "        # Input layer to first hidden layer\n",
    "        prev_size = input_size\n",
    "        for layer_size in hidden_layers:\n",
    "            self.weights.append(np.random.randn(layer_size, prev_size))\n",
    "            self.biases.append(np.zeros((layer_size, 1)))\n",
    "            self.velocities_w.append(np.zeros((layer_size, prev_size)))\n",
    "            self.velocities_b.append(np.zeros((layer_size, 1)))\n",
    "            self.squared_w.append(np.zeros((layer_size, prev_size)))\n",
    "            self.squared_b.append(np.zeros((layer_size, 1)))\n",
    "            self.moments_w.append(np.zeros((layer_size, prev_size)))\n",
    "            self.moments_b.append(np.zeros((layer_size, 1)))\n",
    "            prev_size = layer_size\n",
    "\n",
    "        # Hidden layers to output layer\n",
    "        self.weights.append(np.random.randn(output_size, prev_size))\n",
    "        self.biases.append(np.zeros((output_size, 1)))\n",
    "        self.velocities_w.append(np.zeros((output_size, prev_size)))\n",
    "        self.velocities_b.append(np.zeros((output_size, 1)))\n",
    "        self.squared_w.append(np.zeros((output_size, prev_size)))\n",
    "        self.squared_b.append(np.zeros((output_size, 1)))\n",
    "        self.moments_w.append(np.zeros((output_size, prev_size)))\n",
    "        self.moments_b.append(np.zeros((output_size, 1)))\n",
    "    \n",
    "    def train(self, x_train, y_train, x_val, y_val, epochs, learning_rate, optimizer='sgd', batch_size=32):\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            indices = np.arange(x_train.shape[1])\n",
    "            np.random.shuffle(indices)\n",
    "            x_train, y_train = x_train[:,indices], y_train[:,indices]\n",
    "            \n",
    "            for i in range(0, x_train.shape[1], batch_size):\n",
    "                x_batch = x_train[:,i:i+batch_size]\n",
    "                y_batch = y_train[:,i:i+batch_size]\n",
    "                self.forward(x_batch)\n",
    "                self.backpropagation(x_batch, y_batch, learning_rate, optimizer)\n",
    "        \n",
    "            train_pred, val_pred = self.forward(x_train), self.forward(x_val)\n",
    "            train_loss, val_loss = self.cross_entropy(train_pred, y_train), self.cross_entropy(val_pred, y_val)\n",
    "            train_cls, val_cls = np.argmax(train_pred, axis = 0), np.argmax(val_pred, axis = 0)\n",
    "            train_acc, val_acc = accuracy_score(np.argmax(y_train, axis=0), train_cls), accuracy_score(np.argmax(y_val, axis=0), val_cls)\n",
    "\n",
    "            print(f\"Epoch {epoch}: Training Loss = {train_loss:.4f} Validation Loss = {val_loss:.4f}\")\n",
    "            print(f\"Training Accuracy = {train_acc:.4f} Validation Accuracy = {val_acc:.4f}\")\n",
    "\n",
    "            wandb.log({\n",
    "                \"epoch\" : epoch, \n",
    "                \"train_loss\" : train_loss, \n",
    "                \"val_loss\" : val_loss, \n",
    "                \"train_acc\" : train_acc, \n",
    "                \"val_acc\" : val_acc\n",
    "            })\n",
    "        \n",
    "        wandb.log({\"Accuracy\" : val_acc})\n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def cross_entropy(y_pred, y_true) :\n",
    "        eps = 1e-8 \n",
    "        y_pred = np.clip(y_pred, eps, 1 - eps)\n",
    "        return -np.sum(y_true * np.log(y_pred)) / y_true.shape[1]\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"Sigmoid activation function.\"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def softmax(self, x):\n",
    "        \"\"\"Softmax activation function.\"\"\"\n",
    "        exp_x = np.exp(x - np.max(x, axis=0, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=0, keepdims=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Perform a forward pass through the network.\n",
    "        :param x: Input data.\n",
    "        :return: Output probabilities.\n",
    "        \"\"\"\n",
    "        self.layers = [x]\n",
    "        for w, b in zip(self.weights[:-1], self.biases[:-1]):\n",
    "            x = self.sigmoid(np.dot(w,x) + b)\n",
    "            # x = self.sigmoid(np.dot(x, w) + b)\n",
    "            self.layers.append(x)\n",
    "\n",
    "        # Output layer with softmax\n",
    "        output = self.softmax(np.dot(self.weights[-1], x) + self.biases[-1])\n",
    "        self.layers.append(output)\n",
    "        return output\n",
    "\n",
    "    def backpropagation(self, x, y, learning_rate, optimizer='sgd', beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        m = y.shape[1]  # Number of samples\n",
    "        deltas = [self.layers[-1] - y]  # Output layer error\n",
    "\n",
    "        # Backpropagate errors for hidden layers\n",
    "        for i in range(len(self.weights) - 1, 0, -1):\n",
    "            deltas.append(self.weights[i].T.dot(deltas[-1]) * self.sigmoid_derivative(self.layers[i]))\n",
    "        deltas.reverse()\n",
    "\n",
    "        # Update weights and biases based on optimizer\n",
    "        for i in range(len(self.weights)):\n",
    "            dw = (self.layers[i].dot(deltas[i].T)).T / m\n",
    "            db = np.mean(deltas[i], axis=1, keepdims=True)\n",
    "\n",
    "            if optimizer == 'sgd':\n",
    "                self.weights[i] -= learning_rate * dw\n",
    "                self.biases[i] -= learning_rate * db\n",
    "            \n",
    "            elif optimizer == 'momentum':\n",
    "                self.velocities_w[i] = beta1 * self.velocities_w[i] - learning_rate * dw\n",
    "                self.velocities_b[i] = beta1 * self.velocities_b[i] - learning_rate * db\n",
    "                self.weights[i] += self.velocities_w[i]\n",
    "                self.biases[i] += self.velocities_b[i]\n",
    "            \n",
    "            elif optimizer == 'nag':\n",
    "                prev_w = self.velocities_w[i]\n",
    "                prev_b = self.velocities_b[i]\n",
    "                self.velocities_w[i] = beta1 * prev_w - learning_rate * dw\n",
    "                self.velocities_b[i] = beta1 * prev_b - learning_rate * db\n",
    "                self.weights[i] += -beta1 * prev_w + (1 + beta1) * self.velocities_w[i]\n",
    "                self.biases[i] += -beta1 * prev_b + (1 + beta1) * self.velocities_b[i]\n",
    "            \n",
    "            elif optimizer == 'rmsprop':\n",
    "                self.squared_w[i] = beta2 * self.squared_w[i] + (1 - beta2) * (dw ** 2)\n",
    "                self.squared_b[i] = beta2 * self.squared_b[i] + (1 - beta2) * (db ** 2)\n",
    "                self.weights[i] -= learning_rate * dw / (np.sqrt(self.squared_w[i]) + epsilon)\n",
    "                self.biases[i] -= learning_rate * db / (np.sqrt(self.squared_b[i]) + epsilon)\n",
    "            \n",
    "            elif optimizer == 'adam' or optimizer == 'nadam':\n",
    "                self.velocities_w[i] = beta1 * self.velocities_w[i] + (1 - beta1) * dw\n",
    "                self.velocities_b[i] = beta1 * self.velocities_b[i] + (1 - beta1) * db\n",
    "                self.squared_w[i] = beta2 * self.squared_w[i] + (1 - beta2) * (dw ** 2)\n",
    "                self.squared_b[i] = beta2 * self.squared_b[i] + (1 - beta2) * (db ** 2)\n",
    "                if optimizer == 'nadam':\n",
    "                    dw = beta1 * self.velocities_w[i] + (1 - beta1) * dw\n",
    "                    db = beta1 * self.velocities_b[i] + (1 - beta1) * db\n",
    "                self.weights[i] -= learning_rate * dw / (np.sqrt(self.squared_w[i]) + epsilon)\n",
    "                self.biases[i] -= learning_rate * db / (np.sqrt(self.squared_b[i]) + epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    \"name\": \"Feedforward Network - Hyper parameter search\",\n",
    "    \"metric\": {\n",
    "        \"name\": \"Accuracy\",\n",
    "        \"goal\": \"maximize\"\n",
    "    },\n",
    "    \"method\": \"random\",\n",
    "    \"parameters\": {\n",
    "        \"num_epochs\": {\n",
    "            \"values\": [5, 10]\n",
    "            },\n",
    "        \"num_hiddenLayers\": {\n",
    "            \"values\": [3, 4, 5]\n",
    "            },\n",
    "        \"hiddenLayer_Size\": {\n",
    "            \"values\": [32, 64, 128]\n",
    "            },\n",
    "        \"weightDecay\": {\n",
    "            \"values\": [0, 0.0005, 0.5]\n",
    "            },\n",
    "        \"learningRate\": {\n",
    "            \"values\": [1e-3, 1e-4]\n",
    "            },\n",
    "        \"optimizer\": {\n",
    "            \"values\": [\"sgd\", \"momentum\", \"nesterov\", \"rmsprop\", \"adam\", \"nadam\"]\n",
    "            },\n",
    "        \"batchSize\": {\n",
    "            \"values\": [16, 32, 64]\n",
    "            },\n",
    "        \"weightInit\": {\n",
    "            \"values\": [\"random\", \"xavier\"]\n",
    "            },\n",
    "        \"activationFunc\": {\n",
    "            \"values\": [\"tanh\", \"relu\", \"sigmoid\"]\n",
    "            }\n",
    "    }\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sweep_hyperParameters():\n",
    "    default_config = {\n",
    "        'num_epochs': 10,\n",
    "        'num_hiddenLayers': 3,\n",
    "        'hiddenLayer_Size': 40,\n",
    "        'weightDecay': 0,\n",
    "        'learningRate': 1e-3,\n",
    "        'optimizer': 'Nesterov_Accelerated_GD',\n",
    "        'batchSize': 32,\n",
    "        'weightInit': 'xavier',\n",
    "        'activationFunc': 'sigmoid'\n",
    "    }\n",
    "    \n",
    "    wandb.init(project=\"assignment1\", entity=\"da6401-assignments\")\n",
    "    wandb.init(config = default_config)\n",
    "\n",
    "    config = wandb.config # To get the Hyper Parameters from sweep_config\n",
    "    \n",
    "    # Parameters from wandb.config\n",
    "    num_epochs = config.num_epochs\n",
    "    num_hiddenLayers = config.num_hiddenLayers\n",
    "    hiddenLayer_size = config.hiddenLayer_Size\n",
    "    weightDecay = config.weightDecay\n",
    "    learningRate = config.learningRate\n",
    "    optimizer = config.optimizer\n",
    "    batchSize = config.batchSize\n",
    "    weightInit = config.weightInit\n",
    "    activationFunc = config.activationFunc\n",
    "    \n",
    "    # Name\n",
    "    run_name = f\"LR: {learningRate}, AC: {activationFunc}, BS: {batchSize}, Optim: {optimizer}, WI: {weightInit}, WD: {weightDecay}, No_HL: {num_hiddenLayers}, HS: {hiddenLayer_size}, EP: {num_epochs}\"\n",
    "    print(\"Sweep Name: \",run_name)\n",
    "    \n",
    "    model = FeedforwardNeuralNetwork(x_train.shape[0], [hiddenLayer_size] * num_hiddenLayers, y_train.shape[0])\n",
    "    \n",
    "    wandb.run.name = run_name\n",
    "    \n",
    "    model.train(x_train, y_train, x_val, y_val, num_epochs, learningRate, optimizer, batchSize)\n",
    "\n",
    "    wandb.run.save()\n",
    "\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_ID = wandb.sweep(sweep_config, entity=\"da6401-assignments\", project=\"assignment1\")\n",
    "wandb.agent(sweep_ID, sweep_hyperParameters, count=5)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = wandb.Api()\n",
    "\n",
    "runs = api.runs(\"da6401-assignments/assignment1\")\n",
    "\n",
    "best_run = min(runs, key=lambda run: run.summary.get(\"val_loss\", float(\"inf\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best run name: LR: 0.001, AC: relu, BS: 16, Optim: nadam, WI: xavier, WD: 0, No_HL: 4, HS: 64. \n",
      "Validation Loss: 0.3719278325466944\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best run name: {best_run.name}. \\nValidation Loss: {best_run.summary.get('val_loss')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LR': 0.001, 'AC': 'relu', 'BS': 16, 'Optim': 'nadam', 'WI': 'xavier', 'WD': 0, 'No_HL': 4, 'HS': 64}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "pairs = re.findall(r'(\\w+): ([^,]+)', best_run.name)\n",
    "\n",
    "param_dict = {key: value.strip() for key, value in pairs}\n",
    "\n",
    "param_dict = {\n",
    "    key: int(value) if value.isdigit() else float(value) if value.replace('.', '', 1).isdigit() else value\n",
    "    for key, value in param_dict.items()\n",
    "}\n",
    "\n",
    "print(param_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = FeedforwardNeuralNetwork(input_size= 784, hidden_layers= [param_dict['HS']] * param_dict['No_HL'], output_size= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.train(x_train, y_train, x_val, y_val, \n",
    "                    epochs= param_dict['EP'], \n",
    "                    learning_rate= param_dict['LR'], \n",
    "                    optimizer= param_dict['optim'], \n",
    "                    batch_size= param_dict['BS'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
