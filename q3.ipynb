{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize WandB\n",
    "wandb.init(project=\"assignment1\", entity=\"da6401-assignments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# Normalize the data\n",
    "x_train = x_train.reshape(-1, 784) / 255.0\n",
    "x_test = x_test.reshape(-1, 784) / 255.0\n",
    "\n",
    "# One-hot encode labels\n",
    "y_train, y_test = to_categorical(y_train), to_categorical(y_test)\n",
    "\n",
    "# Split validation data\n",
    "val_split = int(0.1 * x_train.shape[0])\n",
    "\n",
    "# Create validation data\n",
    "x_val, y_val = x_train[:val_split], y_train[:val_split]\n",
    "x_train, y_train = x_train[val_split:], y_train[val_split:]\n",
    "\n",
    "x_train = x_train.T\n",
    "y_train = y_train.T\n",
    "x_val = x_val.T\n",
    "y_val = y_val.T\n",
    "x_test = x_test.T\n",
    "y_test = y_test.T\n",
    "\n",
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardNeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_layers, output_size):\n",
    "        \"\"\"\n",
    "        Initialize the neural network.\n",
    "        :param input_size: Number of input features.\n",
    "        :param hidden_layers: List containing the number of neurons in each hidden layer.\n",
    "        :param output_size: Number of output classes.\n",
    "        \"\"\"\n",
    "        self.layers = []\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.velocities_w = []  # For momentum-based optimizers\n",
    "        self.velocities_b = []\n",
    "        self.squared_w = []  # For RMSprop/Adam\n",
    "        self.squared_b = []\n",
    "        self.moments_w = []  # For Nadam\n",
    "        self.moments_b = []\n",
    "\n",
    "        # Input layer to first hidden layer\n",
    "        prev_size = input_size\n",
    "        for layer_size in hidden_layers:\n",
    "            self.weights.append(np.random.randn(layer_size, prev_size))\n",
    "            self.biases.append(np.zeros((layer_size, 1)))\n",
    "            self.velocities_w.append(np.zeros((layer_size, prev_size)))\n",
    "            self.velocities_b.append(np.zeros((layer_size, 1)))\n",
    "            self.squared_w.append(np.zeros((layer_size, prev_size)))\n",
    "            self.squared_b.append(np.zeros((layer_size, 1)))\n",
    "            self.moments_w.append(np.zeros((layer_size, prev_size)))\n",
    "            self.moments_b.append(np.zeros((layer_size, 1)))\n",
    "            prev_size = layer_size\n",
    "\n",
    "        # Hidden layers to output layer\n",
    "        self.weights.append(np.random.randn(output_size, prev_size))\n",
    "        self.biases.append(np.zeros((output_size, 1)))\n",
    "        self.velocities_w.append(np.zeros((output_size, prev_size)))\n",
    "        self.velocities_b.append(np.zeros((output_size, 1)))\n",
    "        self.squared_w.append(np.zeros((output_size, prev_size)))\n",
    "        self.squared_b.append(np.zeros((output_size, 1)))\n",
    "        self.moments_w.append(np.zeros((output_size, prev_size)))\n",
    "        self.moments_b.append(np.zeros((output_size, 1)))\n",
    "    \n",
    "    def train(self, x_train, y_train, x_val, y_val, epochs, learning_rate, optimizer='sgd', batch_size=32):\n",
    "        for epoch in range(epochs):\n",
    "            indices = np.arange(x_train.shape[1])\n",
    "            np.random.shuffle(indices)\n",
    "            x_train, y_train = x_train[:,indices], y_train[:,indices]\n",
    "            \n",
    "            for i in range(0, x_train.shape[1], batch_size):\n",
    "                x_batch = x_train[:,i:i+batch_size]\n",
    "                y_batch = y_train[:,i:i+batch_size]\n",
    "                self.forward(x_batch)\n",
    "                self.backpropagation(x_batch, y_batch, learning_rate, optimizer)\n",
    "        \n",
    "            train_pred, val_pred = self.forward(x_train), self.forward(x_val)\n",
    "            train_loss, val_loss = self.cross_entropy(train_pred, y_train), self.cross_entropy(val_pred, y_val)\n",
    "            train_cls, val_cls = np.argmax(train_pred, axis = 0), np.argmax(val_pred, axis = 0)\n",
    "            train_acc, val_acc = accuracy_score(np.argmax(y_train, axis=0), train_cls), accuracy_score(np.argmax(y_val, axis=0), val_cls)\n",
    "\n",
    "            print(f\"Epoch {epoch}: Training Loss = {train_loss:.4f} Validation Loss = {val_loss:.4f}\")\n",
    "            print(f\"Training Accuracy = {train_acc:.4f} Validation Accuracy = {val_acc:.4f}\")\n",
    "\n",
    "            wandb.log({\n",
    "                \"epoch\" : epoch, \n",
    "                \"train_loss\" : train_loss, \n",
    "                \"val_loss\" : val_loss, \n",
    "                \"train_acc\" : train_acc, \n",
    "                \"val_acc\" : val_acc\n",
    "            })\n",
    "        \n",
    "        wandb.log({\"Validation Loss\" : val_loss})\n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def cross_entropy(y_pred, y_true) :\n",
    "        eps = 1e-8 \n",
    "        y_pred = np.clip(y_pred, eps, 1 - eps)\n",
    "        return -np.sum(y_true * np.log(y_pred)) / y_true.shape[1]\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"Sigmoid activation function.\"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def softmax(self, x):\n",
    "        \"\"\"Softmax activation function.\"\"\"\n",
    "        exp_x = np.exp(x - np.max(x, axis=0, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=0, keepdims=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Perform a forward pass through the network.\n",
    "        :param x: Input data.\n",
    "        :return: Output probabilities.\n",
    "        \"\"\"\n",
    "        self.layers = [x]\n",
    "        for w, b in zip(self.weights[:-1], self.biases[:-1]):\n",
    "            x = self.sigmoid(np.dot(w,x) + b)\n",
    "            # x = self.sigmoid(np.dot(x, w) + b)\n",
    "            self.layers.append(x)\n",
    "\n",
    "        # Output layer with softmax\n",
    "        output = self.softmax(np.dot(self.weights[-1], x) + self.biases[-1])\n",
    "        self.layers.append(output)\n",
    "        return output\n",
    "\n",
    "    def backpropagation(self, x, y, learning_rate, optimizer='sgd', beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        m = y.shape[1]  # Number of samples\n",
    "        deltas = [self.layers[-1] - y]  # Output layer error\n",
    "\n",
    "        # Backpropagate errors for hidden layers\n",
    "        for i in range(len(self.weights) - 1, 0, -1):\n",
    "            deltas.append(self.weights[i].T.dot(deltas[-1]) * self.sigmoid_derivative(self.layers[i]))\n",
    "        deltas.reverse()\n",
    "\n",
    "        # Update weights and biases based on optimizer\n",
    "        for i in range(len(self.weights)):\n",
    "            dw = (self.layers[i].dot(deltas[i].T)).T / m\n",
    "            db = np.mean(deltas[i], axis=1, keepdims=True)\n",
    "\n",
    "            if optimizer == 'sgd':\n",
    "                self.weights[i] -= learning_rate * dw\n",
    "                self.biases[i] -= learning_rate * db\n",
    "            \n",
    "            elif optimizer == 'momentum':\n",
    "                self.velocities_w[i] = beta1 * self.velocities_w[i] - learning_rate * dw\n",
    "                self.velocities_b[i] = beta1 * self.velocities_b[i] - learning_rate * db\n",
    "                self.weights[i] += self.velocities_w[i]\n",
    "                self.biases[i] += self.velocities_b[i]\n",
    "            \n",
    "            elif optimizer == 'nag':\n",
    "                prev_w = self.velocities_w[i]\n",
    "                prev_b = self.velocities_b[i]\n",
    "                self.velocities_w[i] = beta1 * prev_w - learning_rate * dw\n",
    "                self.velocities_b[i] = beta1 * prev_b - learning_rate * db\n",
    "                self.weights[i] += -beta1 * prev_w + (1 + beta1) * self.velocities_w[i]\n",
    "                self.biases[i] += -beta1 * prev_b + (1 + beta1) * self.velocities_b[i]\n",
    "            \n",
    "            elif optimizer == 'rmsprop':\n",
    "                self.squared_w[i] = beta2 * self.squared_w[i] + (1 - beta2) * (dw ** 2)\n",
    "                self.squared_b[i] = beta2 * self.squared_b[i] + (1 - beta2) * (db ** 2)\n",
    "                self.weights[i] -= learning_rate * dw / (np.sqrt(self.squared_w[i]) + epsilon)\n",
    "                self.biases[i] -= learning_rate * db / (np.sqrt(self.squared_b[i]) + epsilon)\n",
    "            \n",
    "            elif optimizer == 'adam' or optimizer == 'nadam':\n",
    "                self.velocities_w[i] = beta1 * self.velocities_w[i] + (1 - beta1) * dw\n",
    "                self.velocities_b[i] = beta1 * self.velocities_b[i] + (1 - beta1) * db\n",
    "                self.squared_w[i] = beta2 * self.squared_w[i] + (1 - beta2) * (dw ** 2)\n",
    "                self.squared_b[i] = beta2 * self.squared_b[i] + (1 - beta2) * (db ** 2)\n",
    "                if optimizer == 'nadam':\n",
    "                    dw = beta1 * self.velocities_w[i] + (1 - beta1) * dw\n",
    "                    db = beta1 * self.velocities_b[i] + (1 - beta1) * db\n",
    "                self.weights[i] -= learning_rate * dw / (np.sqrt(self.squared_w[i]) + epsilon)\n",
    "                self.biases[i] -= learning_rate * db / (np.sqrt(self.squared_b[i]) + epsilon)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
