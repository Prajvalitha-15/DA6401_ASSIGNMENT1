{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize WandB\n",
    "wandb.init(project=\"assignment1\", entity=\"da6401-assignments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# Normalize the data\n",
    "x_train = x_train.reshape(-1, 784) / 255.0\n",
    "x_test = x_test.reshape(-1, 784) / 255.0\n",
    "\n",
    "# One-hot encode labels\n",
    "y_train, y_test = to_categorical(y_train), to_categorical(y_test)\n",
    "\n",
    "# Split validation data\n",
    "val_split = int(0.1 * x_train.shape[0])\n",
    "\n",
    "# Create validation data\n",
    "x_val, y_val = x_train[:val_split], y_train[:val_split]\n",
    "x_train, y_train = x_train[val_split:], y_train[val_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardNeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_layers, output_size):\n",
    "        \"\"\"\n",
    "        Initialize the neural network.\n",
    "        :param input_size: Number of input features.\n",
    "        :param hidden_layers: List containing the number of neurons in each hidden layer.\n",
    "        :param output_size: Number of output classes.\n",
    "        \"\"\"\n",
    "        self.layers = []\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.velocities_w = []  # For momentum-based optimizers\n",
    "        self.velocities_b = []\n",
    "        self.squared_w = []  # For RMSprop/Adam\n",
    "        self.squared_b = []\n",
    "        self.moments_w = []  # For Nadam\n",
    "        self.moments_b = []\n",
    "\n",
    "        # Input layer to first hidden layer\n",
    "        prev_size = input_size\n",
    "        for layer_size in hidden_layers:\n",
    "            self.weights.append(np.random.randn(prev_size, layer_size) * 0.01)\n",
    "            self.biases.append(np.zeros((1, layer_size)))\n",
    "            self.velocities_w.append(np.zeros((prev_size, layer_size)))\n",
    "            self.velocities_b.append(np.zeros((1, layer_size)))\n",
    "            self.squared_w.append(np.zeros((prev_size, layer_size)))\n",
    "            self.squared_b.append(np.zeros((1, layer_size)))\n",
    "            self.moments_w.append(np.zeros((prev_size, layer_size)))\n",
    "            self.moments_b.append(np.zeros((1, layer_size)))\n",
    "            prev_size = layer_size\n",
    "\n",
    "        # Hidden layers to output layer\n",
    "        self.weights.append(np.random.randn(prev_size, output_size) * 0.01)\n",
    "        self.biases.append(np.zeros((1, output_size)))\n",
    "        self.velocities_w.append(np.zeros((prev_size, output_size)))\n",
    "        self.velocities_b.append(np.zeros((1, output_size)))\n",
    "        self.squared_w.append(np.zeros((prev_size, output_size)))\n",
    "        self.squared_b.append(np.zeros((1, output_size)))\n",
    "        self.moments_w.append(np.zeros((prev_size, output_size)))\n",
    "        self.moments_b.append(np.zeros((1, output_size)))\n",
    "    \n",
    "    def train(self, x_train, y_train, epochs, learning_rate, optimizer='sgd', batch_size=32):\n",
    "        \"\"\"\n",
    "        Train the neural network.\n",
    "        :param x_train: Training data.\n",
    "        :param y_train: Training labels (one-hot encoded).\n",
    "        :param epochs: Number of training iterations.\n",
    "        :param learning_rate: Learning rate.\n",
    "        :param optimizer: Optimization algorithm ('sgd', 'adam', etc.).\n",
    "        :param batch_size: Size of mini-batches for training.\n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            indices = np.arange(x_train.shape[0])\n",
    "            np.random.shuffle(indices)\n",
    "            x_train, y_train = x_train[indices], y_train[indices]\n",
    "            \n",
    "            for i in range(0, x_train.shape[0], batch_size):\n",
    "                x_batch = x_train[i:i+batch_size]\n",
    "                y_batch = y_train[i:i+batch_size]\n",
    "                self.forward(x_batch)\n",
    "                self.backpropagation(x_batch, y_batch, learning_rate, optimizer)\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                loss = -np.mean(y_train * np.log(self.forward(x_train) + 1e-8))\n",
    "                print(f\"Epoch {epoch}: Loss = {loss:.4f}\")\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"Sigmoid activation function.\"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def softmax(self, x):\n",
    "        \"\"\"Softmax activation function.\"\"\"\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Perform a forward pass through the network.\n",
    "        :param x: Input data.\n",
    "        :return: Output probabilities.\n",
    "        \"\"\"\n",
    "        self.layers = [x]\n",
    "        for w, b in zip(self.weights[:-1], self.biases[:-1]):\n",
    "            x = self.sigmoid(np.dot(x, w) + b)\n",
    "            self.layers.append(x)\n",
    "\n",
    "        # Output layer with softmax\n",
    "        output = self.softmax(np.dot(x, self.weights[-1]) + self.biases[-1])\n",
    "        self.layers.append(output)\n",
    "        return output\n",
    "\n",
    "    def backpropagation(self, x, y, learning_rate, optimizer='sgd', beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        \"\"\"\n",
    "        Perform backpropagation and update weights.\n",
    "        :param x: Input data.\n",
    "        :param y: True labels (one-hot encoded).\n",
    "        :param learning_rate: Learning rate for weight updates.\n",
    "        :param optimizer: Optimization algorithm ('sgd', 'rmsprop', 'adam', 'nadam', 'nag').\n",
    "        :param beta1: Momentum parameter for Adam/Nadam.\n",
    "        :param beta2: RMSprop/Adam decay parameter.\n",
    "        :param epsilon: Small value to prevent division by zero.\n",
    "        \"\"\"\n",
    "        m = y.shape[0]  # Number of samples\n",
    "        deltas = [self.layers[-1] - y]  # Output layer error\n",
    "\n",
    "        # Backpropagate errors for hidden layers\n",
    "        for i in range(len(self.weights) - 1, 0, -1):\n",
    "            deltas.append(deltas[-1].dot(self.weights[i].T) * self.sigmoid_derivative(self.layers[i]))\n",
    "        deltas.reverse()\n",
    "\n",
    "        # Update weights and biases based on optimizer\n",
    "        for i in range(len(self.weights)):\n",
    "            dw = self.layers[i].T.dot(deltas[i]) / m\n",
    "            db = np.mean(deltas[i], axis=0, keepdims=True)\n",
    "\n",
    "            if optimizer == 'sgd':\n",
    "                self.weights[i] -= learning_rate * dw\n",
    "                self.biases[i] -= learning_rate * db\n",
    "            \n",
    "            elif optimizer == 'momentum':\n",
    "                self.velocities_w[i] = beta1 * self.velocities_w[i] - learning_rate * dw\n",
    "                self.velocities_b[i] = beta1 * self.velocities_b[i] - learning_rate * db\n",
    "                self.weights[i] += self.velocities_w[i]\n",
    "                self.biases[i] += self.velocities_b[i]\n",
    "            \n",
    "            elif optimizer == 'nag':\n",
    "                prev_w = self.velocities_w[i]\n",
    "                prev_b = self.velocities_b[i]\n",
    "                self.velocities_w[i] = beta1 * prev_w - learning_rate * dw\n",
    "                self.velocities_b[i] = beta1 * prev_b - learning_rate * db\n",
    "                self.weights[i] += -beta1 * prev_w + (1 + beta1) * self.velocities_w[i]\n",
    "                self.biases[i] += -beta1 * prev_b + (1 + beta1) * self.velocities_b[i]\n",
    "            \n",
    "            elif optimizer == 'rmsprop':\n",
    "                self.squared_w[i] = beta2 * self.squared_w[i] + (1 - beta2) * (dw ** 2)\n",
    "                self.squared_b[i] = beta2 * self.squared_b[i] + (1 - beta2) * (db ** 2)\n",
    "                self.weights[i] -= learning_rate * dw / (np.sqrt(self.squared_w[i]) + epsilon)\n",
    "                self.biases[i] -= learning_rate * db / (np.sqrt(self.squared_b[i]) + epsilon)\n",
    "            \n",
    "            elif optimizer == 'adam' or optimizer == 'nadam':\n",
    "                self.velocities_w[i] = beta1 * self.velocities_w[i] + (1 - beta1) * dw\n",
    "                self.velocities_b[i] = beta1 * self.velocities_b[i] + (1 - beta1) * db\n",
    "                self.squared_w[i] = beta2 * self.squared_w[i] + (1 - beta2) * (dw ** 2)\n",
    "                self.squared_b[i] = beta2 * self.squared_b[i] + (1 - beta2) * (db ** 2)\n",
    "                if optimizer == 'nadam':\n",
    "                    dw = beta1 * self.velocities_w[i] + (1 - beta1) * dw\n",
    "                    db = beta1 * self.velocities_b[i] + (1 - beta1) * db\n",
    "                self.weights[i] -= learning_rate * dw / (np.sqrt(self.squared_w[i]) + epsilon)\n",
    "                self.biases[i] -= learning_rate * db / (np.sqrt(self.squared_b[i]) + epsilon)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
